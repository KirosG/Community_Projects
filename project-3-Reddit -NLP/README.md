# Project: Reddit's API WebScrapping for Predicting Comments and Modeling
  Kiros Gebremariam    June 2018
  Data SCience Immersive student @ General Assembly

### Overview
Reddit claims to be the front page of the internet, and that's because they are. With an average of 542 million monthly visitors, of which 234 million are unique, Reddit is one of the top social news distribution and websites rating by users every day from politics to specific daily life. The Reddit website was 6th most visted and ranked here in the united states and accessed globally with high 30's rank. Reddit is subdivided into subreddits, which are themed discussion boards created and populated by Reddit users with links, text, videos and images. These subreddits span an endless array of interests including world news, sports, economics, movies, music, fitness, and more.The site was significantly dependent on users comments and the comments ups(votes). Project three focuses on webscraping using hot/json.  Reddit members discuss proposed topics in the comments section, and the most popular comments are "up-voted" to the top of the discussion board. In 2018, Reddit users submitted nearly more than 70 million posts and followed up with nearly three quarters of a billion comments. With so many users, submissions, comments, and up-votes, it can seem impossible to craft a post that will ever see the light of day. The  Scenario given was i am a fresh Data science bootcamp and looking to break throgh  in the world of freelance data journalism.  Nate Silver and Co at FiveThirtyEight have agreed to hear for a pitch in two weeks.   The goal here was as a Data scientist to determine What characteristics of a post on Reddit are most predictive of the overall interaction on a thread (as measured by number of comments/ups)?
   
   
   -------------------------------------------------------------------------------------------
   Executive Summary
   --------------------------------------------------------------------
  
  This was my first project wherein I had to gather my own data (as opposed to downloading a dataset from Kaggle, for example or the dataset was given in my first project for the statistical analysis of SAT and ACT exams of the 50 US states.) In order to build out my dataset, I scraped 31,975 posts from the "Hot Posts" section of Reddit's website using the 'hot-json requests' and 'Beautiful Soup' libraries. Later on in the project, I once used Reddit's popular API wrapper PRAW to grab some comments from my top 50 'hot' posts. After a few hours (with three second delays in between each set of 1000 posts pulled so as not to be black listed by Reddit's servers), my scrape was complete. I have scrapped 31,975 posts  using the HotJson API for seven days.  
  
  The timing was arranged to be in the morning and afternoon and finally after removing duplicates and conducting the data munging I  have left with only 3300 posts. With any Reddit post, there are upwards of 85 identifying features, and while I did pull all 85 of those features for each of my 3,300 posts, I decided to limit the features that I built into my dataframe to those I thought would be most predictive of popularity. I pared the 85 features down to 17 and stored them in variables. The features I chose to use in my model were: 'author', 'title', 'subreddit', 'subreddit_subscribers', 'score', 'ups', 'downs', 'num_comments', 'title'. If a post had actual original text, it was stored in the 'selftext' feature, however, when I began to explore my dataset in more depth, I learned that many Reddit posts don't actually have original content, but rather, are links to a previous post. With my dataframe finally built out, I was able to start exploring the posts I pulled.The Data collection  and data cleaning took a significant  portion of the time. In the first step, I did some feature analysis. My un-engineed dataset began with 3300 rows and 17 columns. When I grouped by 'subreddit', I identified 1950 unique subreddits within my dataset. While it would be too much to name every single subreddit in this blog post, a few of my favorites were: 'todayilearned', 'mildlyinteresting', and 'individualfinancial'. In exploring the null values of my 'Text' column which corresponded to the 'selftext' feature of my scrape from Reddit, only 416 of posts that I pulled had actual text in the post. The rest were empty and presumably re-posts.  Then I picked up score, title, subreddits and number of comments as my target.  In order to answer and satisfy my goal i reviewed and assesed  variety of models i have been thought in my Data science Immersive course and to determine the best  possible approach to accurately predicting the factors that lead to a popular post , here when i say popular post; its a post defined  with greater than the median number of comments from the collected dataset. After going through all the exploratory data analysis, knowing the multidimensionality of the data elements. I focused on  analysing the data of greatest importance on Reddit I have opted to use a classifier analysis looking at a wide variety of factors.i chosed to see and  investigate the effect of titles on the popularity of a reddit posts.  The stage after  cleaning and munging real data of reddit API, i start manipulating the data for analysis purpose and then after this i scaled the data to check the dimension compatibility and comparability. After going through I then updated my model to gauge the effect of adding several features to the model. Given this, I decided to fill the missing values with 0 in order to keep track of which rows had actual text. With the exception of 'wls' which had 684 missing entries, the rest of my dataset was filled with values. I decided to fill the missing 'wls' values with 6 since that was the mode for the column and sometimes the value of having wls in the metadata was useless also. The score stats directly mirrored the 'ups' stats with a maximum score of 133,613, a mean score of 1,590 and a minimum score of 25. There was nothing to be gleaned from the 'downs' stats given that all of the data were 0s. The comments column, however, was the column I was most interested in, since it made sense that a post's popularity might be greatly correlated with its volume of comments. For the posts that I pulled, the max number of comments was 20,246, the minimum, 0, the mean, 76, and the median, 16. While I did pull these posts from the 'Hot Posts' section from Reddit to begin with, I was tasked with identifying and mapping posts whose comments were above a certain threshold. I used a lambda function to create a new column called 'Hot' which represented posts 16 comments (50% percentile) and above and another column called 'Super Hot' which represented posts with 44 comments (75% percentile) and above.
  
  ## Popular, you're going to be pop-u-lar Really 

I then created a bag of words of all of the words from the 'Title' feature using a for-loop running across the rows in the 'Title feature'. In total, there were 10,445 unique words not including common 'stop' words such as 'the', 'a', & 'and.' I was able to exclude these common 'stop' words using the 'stop_words' arg in my CountVectorizer, specifying 'english' as my language. After performing a train-test-split, I then vectorized the top 20 most common 'Title' words into a sparse dataframe, which I then concatenated onto my X_train and X_test matrices with the hope that more information would improve my model performance. After standardizing my dataset with Standard Scaler, I was ready to run my models. Having fit and scored my models,  In total, I gridsearched over six different models, including a LogisticRegression, an SGDClassifier, a KNeighborsClassifier, a BernoulliNB, a DecisionTreeClassifier, and a RandomForestClassifier. While I remain skeptical of the perfect test scores for the Decision Tree and Random Forest Classifiers, the R2 scores of the other four models seem to make sense. I summarized the results of the models into a summary bar plot, with the bold lines at the top of each bar representing the mean standard deviation of the cross validation runs. 
I chose the logistic regression and  Random Forest Classifier to run feature importances on to understand which of my 32 features were most predictive of 'hot' posts. Unsurprisingly, 'num_comments' was the most predictive feature with a coefficient of 0.761. The next most predictive columns did not have nearly the same predictive power as num_comments, with 'score', 'ups', and 'subreddit_subscribers' as the next most predictive with coefficients of 0.0863, 0.0725, and 0.0247, respectively.

 I have applied count vectorizer and found the most common words as well as i found that posts with less than 40 characters were the most commented by the reddit users.  As reddit is one of the top three most visited website  in the united states focusing on short titles and using  the third quantile of the number of comments by subreddit, i found out that Askreddit was the most visited site followed by worldnews,technologysubreddit,personalfinance and videos.  Finally, I have picked  Logistic regression,RandomForest Classifier as my model. I found a model that accurately predicts the whether or not a post will be popular with about  70-80% accuracy on a consistent basis. My predictive features (title,subreddit, num_comments) were highly effective in other models as well. Ultimately the most effective model selected in my analysis was LogisticRegressionClassifier and random forest classifier. My findings suggest that this is a highly effective model which has the potential for further analysis both within the reddit sight. I believe this analysis can also be replicated in the same site or other similar websites to other post base discussion and social media sites. 
  
------------------------
## Presentation: 
Presentation linke:  https://docs.google.com/presentation/d/1aR8oIEEyCnFE8EqKGk96u-IzMyF8dCXpBWUWF69ymfY/edit#slide=id.g3c20741785_2_144
----------------------------
## DATA SOURCE
*  url = "https://www.reddit.com/hot.json
* collected for almost 7 days
* Collected 31,975 posts
* Reduced to 3,300 unique
* Every Day collected data was concatenated  until the last day
*  The memory size of the Excel CSV files exceeded more than 1.5 GB
------------------------------
## DATA COMPONENTS
* TITTLE 
* Subreddit
* ASKSUBREDDIT
* NUMBER OF COMMENTS

--------------
#### DATA SCIENCE TOOLS AND METHODS APPLIED

### Natural Language Processing
* I applied Countvectorizer and TFIDF “vectorization” the process of turning text into numbers so that we can predict an outcome. Using these i have generated some out puts and can be accessed on 
* Using this count vectorization  i applied randomforest and logistic regression to predict what text will engage readers most 

## MACHINE LEARNING (PREDICTION/Modeling)
* Random Forest
* Logistic Regression
* KNN
*  The Logistic Regression performed better than RF and KNN

